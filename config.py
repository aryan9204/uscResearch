llama_config = {
    "config_list": [
        {
            "model": "llama3:8b",
            "base_url": "http://127.0.0.1:11438/v1",
            "api_key": "ollama",
            "price": [0, 0],
        },
    ],
    "cache_seed": None,
    "temperature": 1,
}
